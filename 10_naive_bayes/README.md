# Naive Bayes - Machine Learning e Inferencia Bayesiana

Esta es la secci贸n sobre el modelo **Naive Bayes**, uno de los algoritmos de clasificaci贸n m谩s elegantes y fundamentales del machine learning, basado en el famoso teorema de Bayes.

## 驴Qu茅 vas a aprender?

El clasificador Naive Bayes es perfecto para entender c贸mo las matem谩ticas se convierten en predicciones 煤tiles. A lo largo de esta secci贸n, vas a:

- **Comprender la teor铆a**: Desde el teorema de Bayes hasta la "ingenua" suposici贸n de independencia
- **Implementar desde cero**: Crear tu propio clasificador para entender cada paso del proceso
- **Aplicar en casos reales**: Trabajar con datasets del mundo real como detecci贸n de spam y supervivencia del Titanic
- **Comparar variantes**: Explorar MultinomialNB, BernoulliNB y CategoricalNB de scikit-learn

## Contenido de la secci贸n

###  Materiales disponibles

- **`10_naive_bayes_slides.pdf`**: Presentaci贸n te贸rica con los fundamentos matem谩ticos
- **`10_naive_bayes_spam_ficticio.ipynb`**: Implementaci贸n manual paso a paso con datos sint茅ticos
- **`10_naive_bayes_titanic_comparacion_manual.ipynb`**: Comparaci贸n entre implementaci贸n manual y scikit-learn
- **`10_naive_bayes_spam_real.ipynb`**: Aplicaci贸n completa con dataset real de SMS spam

###  Casos de estudio

#### 1. Detecci贸n de SPAM (Datos ficticios)
Empezar谩s con un ejemplo sencillo donde implementar谩s el algoritmo desde cero. Vas a:
- Entender c贸mo calcular probabilidades condicionales
- Aplicar el teorema de Bayes manualmente
- Ver c贸mo se toman las decisiones de clasificaci贸n

#### 2. Supervivencia del Titanic
Un cl谩sico del machine learning donde comparar谩s:
- Tu implementaci贸n manual vs. scikit-learn
- Diferentes variantes del algoritmo (Multinomial, Bernoulli, Categorical)
- M茅tricas de evaluaci贸n y matrices de confusi贸n

#### 3. Detecci贸n de SPAM (Datos reales)
El proyecto m谩s completo donde trabajar谩s con:
- **5,572 mensajes SMS reales** etiquetados como spam/ham
- Preprocesamiento de texto (limpieza, stopwords, vectorizaci贸n)
- Optimizaci贸n de hiperpar谩metros
- Evaluaci贸n exhaustiva del modelo

## Conceptos clave

### Fundamentos te贸ricos
- **Teorema de Bayes**: P(A|B) = P(B|A)  P(A) / P(B)
- **Suposici贸n de independencia**: Por qu茅 es "naive" pero efectivo
- **Probabilidades a priori y posteriori**
- **Verosimilitud (likelihood)**

### Aspectos pr谩cticos
- **Preprocesamiento de texto**: Tokenizaci贸n, limpieza, vectorizaci贸n
- **Manejo de stopwords**: Eliminaci贸n de palabras comunes
- **Suavizado (smoothing)**: Evitar probabilidades cero
- **Evaluaci贸n de modelos**: Accuracy, precision, recall, matriz de confusi贸n

### Variantes del algoritmo
- **MultinomialNB**: Para datos de conteo (frecuencia de palabras)
- **BernoulliNB**: Para datos binarios (presencia/ausencia)
- **CategoricalNB**: Para variables categ贸ricas
- **GaussianNB**: Para datos continuos (no cubierto en esta secci贸n)

## Flujo de trabajo recomendado

1. **Comienza con la teor铆a**: Revisa las slides para entender los fundamentos
2. **Implementaci贸n b谩sica**: Trabaja el notebook de spam ficticio
3. **Comparaci贸n pr谩ctica**: Explora el caso del Titanic
4. **Proyecto completo**: Finaliza con el dataset real de SMS

## Resultados esperados

Al completar esta secci贸n, habr谩s:

**Implementado** tu propio clasificador Naive Bayes desde cero  
**Aplicado** el algoritmo a problemas reales de clasificaci贸n  
**Comparado** diferentes variantes y sus casos de uso  
**Evaluado** modelos usando m茅tricas apropiadas  
**Procesado** texto para tareas de NLP b谩sicas  

## Tips para el 茅xito

- **No te saltes la implementaci贸n manual**: Es donde realmente entiendes el algoritmo
- **Experimenta con los hiperpar谩metros**: Especialmente el par谩metro alpha (smoothing)
- **Analiza las matrices de confusi贸n**: Te dan insights valiosos sobre el rendimiento
- **Compara siempre**: Manual vs. scikit-learn, diferentes variantes entre s铆